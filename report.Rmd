---
title: "XGBoost-based used electric car price prediction"
author: "Zheyan Liu, Zexu Yuan, Baode Gao"

output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
editor_options: 
  chunk_output_type: console
  


--- 

\newpage


```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(patchwork)
library(leaflet)
library(knitr)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .7,
  out.width = "95%"
)

theme_set(theme_minimal() + theme(legend.position = 'bottom'))

options(
  ggplot2.continuous.colour = 'viridis',
  ggplot2.continuous.fill = 'viridis'
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Introduction

```{r, include=FALSE}
df_raw = read_csv('data/vehicles.csv', show_col_types = FALSE)
df = 
  df_raw %>% 
  filter(fuel == 'electric') %>% 
  filter(
      # price > quantile(price,0.02),
      # price < quantile(price,0.98)
    )

df1 = 
df %>% 
    select(price, year, manufacturer, condition, odometer, title_status, transmission,drive, type, paint_color, state, lat, long) 

# as.data.frame(sapply(X = df2, FUN = function(x) sum(is.na(x)))/nrow(df2))  %>% knitr::kable(col.names = "Missing rate")
# 
# range(df %>% drop_na(long) %>% pull(long))
# 
# str(df2)
# 
# length(table(df2$paint_color))
# 
# table(df2$paint_color)
```


As the oil price rises, more and more people begin to pay attention to electric cars. First of all, electric cars can save money. No matter where you charge in the U.S., electric cars are cheaper to fuel than gasoline-powered vehicles. Then, electric cars can reduce emissions. Today, the average U.S. electric vehicle emits as much as a gasoline vehicle that gets 73 miles per gallon. As wind and solar power replace coal-fired generation, the emissions performance of electric vehicles will improve. Moreover, electric vehicles offer a better driving experience. Electric engines produce instant torque, which means electric vehicles can narrow the starting line and provide smooth, responsive acceleration and deceleration. Electric vehicles have a lower center of gravity, which improves handling, responsiveness and ride comfort. Rising energy cost would contribute to higher prices of vehicles including second-hand cars; however, electric vehicles, an alternative to traditional motor vehicles, recently plays an increasingly important role in used car market. Basing on that, more families might prefer electric cars. This paper will work on building models to help predict the used electric car price so that customers can use this model to determine whether the deal is reasonable. The report is within five pages excluding contents, plots and tables, check out the excluded version **[here](https://github.com/zheyanliu99/craigslist_used_cars/blob/main/report_noplot.pdf)**


We used data from **[Craigslist](https://newyork.craigslist.org/)**, which is the world's largest collection of used vehicles for sale. The **[original data](https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data)** contains price of used car from Apr 2021 to May 2021, it contains `r nrow(df_raw)` observations and  `r ncol(df_raw)` variables. Since we are only interested in cars fueled by electricity, data is reduced to `r nrow(df)` observations.


## Research Questions

* Find appropriate way to handle missing values.

* Conduct exploratory data analysis to find interesting facts about the data.

* Build and compare machine learning models to find the best one for the prediction task.

## Data preparation

We drop variables that has missing rate higher than 35%. For variables with relatively high missing rate (>2%), we analyze the missing pattern, whether they are MAR or MNAR. For MNAR categorical variables, missingness is treat as an attribute *NAN_cat*. For MAR categorical variables, missing values are imputed with mode. After selection and imputation, the final variables for model are as follow

| Variable     | Type       | levels/range   | Missing rate | Missing type | Impute method   |
|--------------|------------|----------------|--------------|--------------|-----------------|
| Price        | continuous | 0-130000       | 0            | NA           | NA              |
| Year         | continuous | 1901-2022      | 0.2%         | MAR          | Median          |
| Odometer     | continuous | 0-1111111      | 0.3%         | MAR          | Median          |
| Lat          | continuous | 19.64-61.57    | 1.8%         | MAR          | Median          |
| Long         | continuous | -159.37--70.06 | 1.8%         | MAR          | Median          |
| Manufacturer | category   | 29 levels      | 4.8%         | MNAR         | NA as attribute |
| Condition    | category   | 6 levels       | 31.6%        | MNAR         | NA as attribute |
| Title_status | category   | 6 levels       | 1.5%         | MNAR         | NA as attribute |
| Transmission | category   | 3 levels       | 1.1%         | MAR          | Mode            |
| Drive        | category   | 3 levels       | 19.9%        | MAR          | Mode            |
| Type         | category   | 11 levels      | 10.1%        | MAR          | Mode            |
| State        | category   | 49 levels      | 0            | NA           | NA              |
| Paint_color  | category   | 11 levels      | 26.4%        | MAR          | Mode            |

*Manufacturer*, *Title_status* and *Condition* are considered MNAR because the records with missing values clearly has lower price compared to other category.

# Exploratory data analysis

We discovered some interesting facts through visualization. Note that all the exploratory data analysis are based on the raw data without imputation.

## Interesting price distribution


```{r, fig.asp=0.6, echo=FALSE}
p1 = 
  df %>% 
    ggplot(aes(x=price)) + geom_histogram(bins=35,color="darkorchid4", fill="darkorchid4") +
    labs(y="count", x="price of all cars")

p2 = 
  df %>% 
    filter(manufacturer=='tesla') %>% 
    ggplot(aes(x=price)) + geom_histogram(bins=30,color="darkgoldenrod2", fill="darkgoldenrod2")+ 
    labs(y="count", x="price of Tesla")

p1 + p2
```

From the first price histogram, there is clearly two vertices of the price, and the price is relatively skewed to the left. The reason is that 689 out of 1698 observations in the dataset is manufacturered by Tesla, and Tesla has a higher price than most of other brands. 

In addition, there are some errorness in the dataset, it contains some prices equal or very close to 0. We remove the 7% lowest price records in the dataset (7% quantile on price is `r quantile(df$price, 0.07)`).

## Price vs category variables

```{r, fig.height = 32, fig.width = 8, fig.asp=1, echo=FALSE}
p1 = 
  df1 %>% 
    group_by(paint_color) %>% 
    mutate(n = n()) %>% 
    drop_na(paint_color) %>% 
    filter(n >= 30) %>% 
    ggplot(aes(x=reorder(paint_color, price, median, na.rm = TRUE), y=price, fill=paint_color)) + 
    geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
    theme(legend.position = "none") + 
    labs(y="price", x="paint_color")

p2 = 
  df1 %>% 
    group_by(type) %>% 
    mutate(n = n()) %>% 
    drop_na(type) %>% 
    filter(n >= 30) %>% 
    ggplot(aes(x=reorder(type, price, median, na.rm = TRUE), y=price, fill=type)) + 
    geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
    theme(legend.position = "none") + 
    labs(y="price", x="type")

p3 = 
  df1 %>% 
    group_by(manufacturer ) %>% 
    mutate(n = n()) %>% 
    drop_na(manufacturer ) %>% 
    filter(n >= 30) %>% 
    ggplot(aes(x=reorder(manufacturer , price, median, na.rm = TRUE), y=price, fill=manufacturer)) + 
    geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
    theme(legend.position = "none") + 
    labs(y="price", x="manufacturer ")

p4 = 
  df1 %>% 
    group_by(condition) %>% 
    mutate(n = n()) %>% 
    drop_na(condition) %>% 
    filter(n >= 30) %>% 
    ggplot(aes(x=reorder(condition , price, median, na.rm = TRUE), y=price, fill=condition)) + 
    geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
    theme(legend.position = "none") + 
    labs(y="price", x="condition")

p5 = 
  df1 %>% 
    group_by(drive) %>% 
    mutate(n = n()) %>% 
    drop_na(drive) %>% 
    filter(n >= 30) %>% 
    ggplot(aes(x=reorder(drive  , price, median, na.rm = TRUE), y=price, fill=drive)) + 
    geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
    theme(legend.position = "none") + 
    labs(y="price", x="drive ")

p6 = 
  df1 %>% 
    group_by(transmission) %>% 
    mutate(n = n()) %>% 
    drop_na(transmission) %>% 
    filter(n >= 30) %>% 
    ggplot(aes(x=reorder(transmission, price, median, na.rm = TRUE), y=price, fill=transmission)) + 
    geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
    theme(legend.position = "none") + 
    labs(y="price", x="transmission")

p7 = 
  df1 %>% 
    group_by(state) %>% 
    mutate(n = n()) %>% 
    drop_na(state) %>% 
    filter(n >= 30) %>% 
    ggplot(aes(x=reorder(state, price, median, na.rm = TRUE), y=price, fill=state)) + 
    geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
    theme(legend.position = "none") + 
    labs(y="price", x="state")

(p1 + p2 + p3)/(p4 + p5+ p6)/p7
```

Despite some common knowledge, here is some interesting factors from these boxplot:

* White cars has the highest price while red ones has the lowest median price. However, prices of red cars are scattered.
* 4wd cars has the highest price and the reason behind this ican be car type. 123 out of 166 4wd cars are SUV and Sedan.
* Electric cars New Jersy has the highest median price while California is one of the states with the lowest median price.

## Price map

<center>

![](price_map.png)

</center>

Most car sales takes place near the Coast or the Great Lakes Region. In addition, the car price in the East Coast is clearly higher than that in the West Coast

# Models

We used Lasso, Regression Tree and Gradient Boosting Tree to predict the price. We used Lasso because there is a considerable number of variables (239 including dummy variables) in the training data. And L1 regularization can help reduce dimension and avoid multicollinearity. In addition, We selected Regression Tree because it is easy to interpret and it captures the interaction between variables. Finally, We adopted the ensemble model GBM to better utilize the good property of tree-based models, a single tree can have high bias while boosting methods fits the residual of last round to gradually reduce bias.

## Model preparation

Conduct model preparation with exact following steps

* Impute the data and divide the data into train set and test set. Test set takes up 20%.
* Using MinMaxScaler to scale all continuous variable in range [0, 1] so that the Lasso coefficients are comparable.
* Remove the 2% records with low price in the training set.

```{r, echo=FALSE}
calc_mode <- function(x){
  
  # List the distinct / unique values
  distinct_values <- unique(x)
  
  # Count the occurrence of each distinct value
  distinct_tabulate <- tabulate(match(x, distinct_values))
  
  # Return the value with the highest occurrence
  distinct_values[which.max(distinct_tabulate)]
}

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}


df1 = 
  df %>% 
    select(price, year, manufacturer, condition, odometer, title_status, transmission,drive, type, paint_color, state, lat , long) %>% 
    mutate(
      manufacturer = ifelse(is.na(manufacturer), 'NAN_cat', manufacturer),
      # odometer = ifelse(is.na(odometer), mean(odometer, na.rm=TRUE), odometer),
      title_status = ifelse(is.na(title_status), 'NAN_cat', title_status)
      # year = ifelse(is.na(year), mean(year, na.rm=TRUE), year),
    ) %>% 
    mutate(
      condition = as.factor(condition),
      condition = fct_reorder(condition, price, median),
      
      title_status = as.factor(title_status),
      title_status = fct_reorder(title_status, price, median),
      
      manufacturer = as.factor(manufacturer),
      manufacturer = fct_reorder(manufacturer, price, median),
      
      transmission = as.factor(transmission),
      transmission = fct_reorder(transmission, price, median),
      
      drive = as.factor(drive),
      drive = fct_reorder(drive, price, median),
      
      type = as.factor(type),
      type = fct_reorder(type, price, median),
      
      paint_color = as.factor(paint_color),
      paint_color = fct_reorder(paint_color, price, median),
      
      state = as.factor(state),
      state = fct_reorder(state, price, median)
           ) %>% 
    filter(
    price > quantile(df$price, 0.07)
  )


set.seed(777)
# Create Train and Test
indexTrain = 
  createDataPartition(
    y = df1$price,
    p = 0.8,
    list = FALSE)

train_df = df1[indexTrain,]
train_df2 = train_df
train_df = 
  train_df %>% 
    mutate(
      # category impute
      condition = replace_na(condition, calc_mode(train_df %>% filter(!is.na(condition))  %>%  pull(condition))),
      title_status = replace_na(title_status, calc_mode(train_df %>% filter(!is.na(title_status))  %>%  pull(title_status))),
      transmission = replace_na(transmission, calc_mode(train_df %>% filter(!is.na(transmission))  %>%  pull(transmission))),
      drive = replace_na(drive, calc_mode(train_df %>% filter(!is.na(drive))  %>%  pull(drive))),
      type = replace_na(type, calc_mode(train_df %>% filter(!is.na(type))  %>%  pull(type))),
      paint_color = replace_na(paint_color, calc_mode(train_df %>% filter(!is.na(paint_color))  %>%  pull(paint_color))),
      
      # continuous impute
      odometer = ifelse(is.na(odometer), median(train_df$odometer, na.rm=TRUE), odometer),
      year = ifelse(is.na(year), median(train_df$year, na.rm=TRUE), year),
      lat = ifelse(is.na(lat), median(train_df$lat, na.rm=TRUE), lat),
      long = ifelse(is.na(long), median(train_df$long, na.rm=TRUE), long),
      
      # # continuous MinMaxScalar
      # odometer = normalize(odometer),
      # year = normalize(year),
      # lat = normalize(lat),
      # long = normalize(long)) 
    )
  

test_df = df1[-indexTrain,]
test_df = 
  test_df %>% 
    mutate(
      # category impute
      condition = replace_na(condition, calc_mode(train_df2 %>% filter(!is.na(condition))  %>%  pull(condition))),
      title_status = replace_na(title_status, calc_mode(train_df2 %>% filter(!is.na(title_status))  %>%  pull(title_status))),
      transmission = replace_na(transmission, calc_mode(train_df2 %>% filter(!is.na(transmission))  %>%  pull(transmission))),
      drive = replace_na(drive, calc_mode(train_df2 %>% filter(!is.na(drive))  %>%  pull(drive))),
      type = replace_na(type, calc_mode(train_df2 %>% filter(!is.na(type))  %>%  pull(type))),
      paint_color = replace_na(paint_color, calc_mode(train_df2 %>% filter(!is.na(paint_color))  %>%  pull(paint_color))),
      
      # continuous impute
      odometer = ifelse(is.na(odometer), median(train_df2$odometer, na.rm=TRUE), odometer),
      year = ifelse(is.na(year), median(train_df2$year, na.rm=TRUE), year),
      lat = ifelse(is.na(lat), median(train_df2$lat, na.rm=TRUE), lat),
      long = ifelse(is.na(long), median(train_df2$long, na.rm=TRUE), long),
      
      # continuous MinMaxScalar
      # odometer = normalize(odometer),
      # year = normalize(year),
      # lat = normalize(lat),
      # long = normalize(long)) 
)

train_df = 
  train_df %>% 
      filter(
      price > quantile(df1$price, 0.01),
      price < quantile(df1$price, 0.99))

```

## Building model and tuning parameters

Use cross validation to select the best parameter or parameter combination for each model.

### Lasso


```{r, echo=FALSE}
lasso.fit <- train(price~.,
                   data = train_df,
                   method = "glmnet",
                   metric = 'RMSE',
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(6, -2, length=300))),
                   trControl = trainControl(method = "cv"))

```

The parameter $\lambda$ controls the L1 Regularization, the bigger the $\lambda$, the fewer variables in the model. Set the candidate values of $\lambda$ to be from `r exp(-2)` to `r exp(6)` with 300 steps, the best-tune $\lambda$ is `r lasso.fit$bestTune[,2]`.

### Regression Tree


```{r, echo=FALSE}
tree.fit <- train(price~.,
                   data = train_df,
                   method = "rpart2", 
                   tuneLength = 10, 
    trControl = trainControl(method = "cv"))

```

The parameter *max tree depth* determines how many splits/leaves the tree can get. A lower *max tree depth* may result in underfitting while a higher *max tree depth* can lead to overfitting. Set the candidate values of parameter to be 1 to 10 with step of 1, the  best-tune *max tree depth* is `r tree.fit$bestTune[,1]`

### Gradient Boosting Regression Tree



```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
grid<-expand.grid(.n.trees=seq(200,500,by=100),.interaction.depth=seq(2,7,by=1),.shrinkage=c(.05,.1),
                  .n.minobsinnode=10)
grid<-expand.grid(.n.trees=seq(500,500,by=100),.interaction.depth=seq(4,7,by=1),.shrinkage=c(.1),
                  .n.minobsinnode=10)


gbm.train<-train(price~.,
                 data=train_df,
                 method='gbm',
                 trControl=trainControl(method = "cv"),
                 tuneGrid=grid)

gbm.train$bestTune[, 2]
```

There are several parameters in the GBM model. *num of trees* controls the number of estimators/base-trees in the ensemble model.*interaction depth* is similar to *max tree depth* in the Regression Tree, it determines the highest level of variable interactions allowed while training the model.  *shrinkage* is considered as the learning rate. It is used for reducing, the impact of each additional fitted base-tree. For *num of trees* and *interaction depth*, small value may cost underfitting and the bigger one can result in overfitting while *shrinkage* does just the opposite.

Set the range for *num of trees* to be 200 to 500 with step of 100, the range for *interaction depth* to be 2 to 7 with step of 1 and *shrinkage* to be 0.05 or 0.1. The best-tune *num of trees* is `r gbm.train$bestTune[, 1]`, *interaction depth* `r gbm.train$bestTune[, 2]` is and *shrinkage* is `r gbm.train$bestTune[, 3]`

### XGBoost

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
set.seed(777)
library(xgboost)
library(pdp)
library(vip) # install.packages("vip")

xgbCtrl = trainControl(method = "cv", number = 10)

xgbGrid <- expand.grid(nrounds = c(500, 1000, 1500),
                       eta = c(0.01, 0.05),
                       max_depth = c(2, 4, 6),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       subsample = c(0.5, 1),
                       gamma = c(0, 50),
                       min_child_weight = c(0, 20)
                       )

# simple grid
xgbGrid <- expand.grid(nrounds = c(1000),
                       eta = c(0.05),
                       max_depth = c(4),
                       colsample_bytree = 0.6,
                       subsample = 0.9,
                       gamma = 0,
                       min_child_weight = 2
                       )

xgb.fit <- train(price ~ ., 
                 train_df, 
                 method = "xgbTree",
                 objective = "reg:squarederror",
                 trControl = xgbCtrl,
                 tuneGrid = xgbGrid
                 )

xgb.fit$bestTune

# plot(xgb.fit, highlight = TRUE)

xgb.pred <- predict(xgb.fit, newdata = test_df, type = "raw")

xgb.rmse <- sqrt(mean((test_df$price - xgb.pred)^2))

xgb.rmse

xgb_var <- xgboost(data = data.matrix(subset(train_df, select = -price)),
                   label = train_df$price, 
                   objective = "reg:squarederror",
                   nrounds = xgb.fit$bestTune[[1]], 
                   max_depth = xgb.fit$bestTune[[2]], 
                   eta = xgb.fit$bestTune[[3]], 
                   gamma = xgb.fit$bestTune[[4]],
                   colsample_bytree = xgb.fit$bestTune[[5]],
                   min_child_weight = xgb.fit$bestTune[[6]],
                   subsample = xgb.fit$bestTune[[7]])

# vip(xgb_var, num_features = 10)
bestTune = xgb.fit$bestTune
```

*nrounds* is the number of decision trees in the final model -- 500, 1000, 1500 were selected as options; 
*eta* is analogous to learning rate in the model, makin the model more robust by shrinking weights -- 0.01 and 0.05 were seleced; *max_depth* is the max depth of a tree where higher depth will allow model to learn relations very specific to a particular sample -- 2, 4, 6 were selected as options; 
*colsample_bytree* denotes the fraction of columns to be randomly samples for each tree -- the selection is from 0.5 to 0.9 with 0.08 as one step; 
*subsample* denotes the fraction of observations to be randomly samples for each tree -- 0.5 and 1 were selected as options; 
*gamma* specifies the minimum loss reduction required to make a split -- 0 and 50 were selected as options; 
*min_child_weight* is the minimum sum of instance weight needed in a child -- 0 and 20 were selected as options.

The optimal *nrounds* is `r bestTune[[1]]`, optimal *max_depth* is `r bestTune[[2]]`, optimal *eta* is `r bestTune[[3]]`, optimal *gamma* is `r bestTune[[4]]`, optimal *colsample_bytree* is `r bestTune[[5]]`, optimal *min_child_weight* is `r bestTune[[6]]`, and the optimal *subsample* is `r bestTune[[7]]`.

RMSE for XGBoost is `r xgb.rmse`.

## Model performance

```{r, fig.height = 10, fig.width = 8, fig.asp=1, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(777)
res = 
  resamples(list(LASSO = lasso.fit,
                 TREE = tree.fit,
                 GBM = gbm.train,
                 XGB= xgb.fit))


cv_res = res$values
p1 = 
  cv_res %>% 
    as.tibble() %>% 
    select('LASSO~RMSE', 'TREE~RMSE', 'GBM~RMSE', 'XGB~RMSE') %>% 
    pivot_longer(c('LASSO~RMSE', 'TREE~RMSE', 'GBM~RMSE', 'XGB~RMSE'), 
                 names_to = 'model',
                 values_to = 'RMSE') %>% 
    mutate(
      model = ifelse(model=='LASSO~RMSE', 'LASSO', model),
      model = ifelse(model=='TREE~RMSE', 'TREE', model),
      model = ifelse(model=='GBM~RMSE', 'GBM', model),
      model = ifelse(model=='XGB~RMSE', 'XGB', model),
      model = as.factor(model),
      model = fct_reorder(model, -RMSE, median)
    ) %>% 
    ggplot(aes(x=model, y=RMSE, fill = model)) + theme(plot.title = element_text(size = 10, face = "bold")) +
  geom_boxplot() + ggtitle('Cross Validation RMSE')

pred_y = predict(lasso.fit, newdata = as.tibble(test_df))
rmse_lasso_original = sqrt(mean((pred_y - test_df$price)^2)) 

pred_y = as.tibble(pred_y)
pred_y = 
  pred_y %>% 
  mutate(value = ifelse(value < 0 ,  0, value))
rmse_lasso = sqrt(mean((pred_y$value - test_df$price)^2)) 


y_compare_df = 
  tibble(pred = pred_y$value,
         actual = test_df$price) %>% 
  arrange(actual) %>% 
    pivot_longer(
      cols = pred:actual,
      names_to = 'type',
      values_to = 'price'
    ) 

p2 = 
y_compare_df %>% 
  mutate(
    num = seq(1, nrow(y_compare_df)),
    type = as.factor(type)
         ) %>% 
  ggplot(aes(x = num, y = price), color = type) + 
  geom_point(aes(colour = type)) + 
  theme(plot.title = element_text(size = 10, face = "bold")) + ggtitle(paste0('Testset RMSE of LASSO:', as.character(as.integer(rmse_lasso))))


pred_y = predict(tree.fit, newdata = as.tibble(test_df))
rmse_tree = sqrt(mean((pred_y - test_df$price)^2)) 

y_compare_df = 
  tibble(pred = pred_y,
         actual = test_df$price) %>% 
  arrange(actual) %>% 
    pivot_longer(
      cols = pred:actual,
      names_to = 'type',
      values_to = 'price'
    ) 

p3 = 
y_compare_df %>% 
  mutate(
    num = seq(1, nrow(y_compare_df)),
    type = as.factor(type)
         ) %>% 
  ggplot(aes(x = num, y = price), color = type) + 
  geom_point(aes(colour = type)) + 
  theme(plot.title = element_text(size = 10, face = "bold")) + 
  ggtitle(paste0('Testset RMSE of TREE:', as.character(as.integer(rmse_tree))))

library(gbm)
pred_y = predict(gbm.train, newdata = as.tibble(test_df))
rmse_gbm = sqrt(mean((pred_y - test_df$price)^2)) 


y_compare_df = 
  tibble(pred = pred_y,
         actual = test_df$price) %>% 
  arrange(actual) %>% 
    pivot_longer(
      cols = pred:actual,
      names_to = 'type',
      values_to = 'price'
    ) 

p4 = 
y_compare_df %>% 
  mutate(
    num = seq(1, nrow(y_compare_df)),
    type = as.factor(type)
         ) %>% 
  ggplot(aes(x = num, y = price), color = type) + 
  geom_point(aes(colour = type)) + 
  theme(plot.title = element_text(size = 10, face = "bold")) + 
  ggtitle(paste0('Testset RMSE of GBM:', as.character(as.integer(rmse_gbm))))

pred_y = predict(gbm.train, newdata = as.tibble(test_df))
rmse_gbm = sqrt(mean((pred_y - test_df$price)^2)) 

# XGB
pred_y = predict(xgb.fit, newdata = as.tibble(test_df))
rmse_xgb = sqrt(mean((pred_y - test_df$price)^2)) 
y_compare_df = 
  tibble(pred = pred_y,
         actual = test_df$price) %>% 
  arrange(actual) %>% 
    pivot_longer(
      cols = pred:actual,
      names_to = 'type',
      values_to = 'price'
    ) 

p5 = 
y_compare_df %>% 
  mutate(
    num = seq(1, nrow(y_compare_df)),
    type = as.factor(type)
         ) %>% 
  ggplot(aes(x = num, y = price), color = type) + 
  geom_point(aes(colour = type)) + 
  theme(plot.title = element_text(size = 10, face = "bold")) + 
  ggtitle(paste0('Testset RMSE of XGB:', as.character(as.integer(rmse_xgb))))



p1/(p2 + p3) / (p4 + p5)

```

The full training performances are shown below:

|TRAIN RESULTS | MAE (median)| RMSE (median)| RSQUARED (median)|
|--------------|-------------|--------------|------------------|
| LASSO        | 4251.711    | 6680.215     | 0.8031168        | 
| TREE         | 3982.900    | 6054.306     | 0.8325880        |
| GBM          | 2400.654    | 4390.029     | **0.9146599**    | 
| XGB          | **2102.903**| **4219.979** | 0.9136081        | 

The cross validation median RMSE LASSO(6680) > TREE(6054) >> GBM(4390) > XGB(4219.979). So, the XGB should be seleceted as final model. The reason is that most variables do not have linear relationship with price. Additionally, Lasso model does not allow interaction between variables while Tree-based model considers that. 

What is more, we plotted the scatterplot of actual and predicted value. Note that Lasso can predict the price to be negative values, which is not reasonable in practice. we used a simple function $price_{pred}=max(0,price_{pred})$ to correct that. On the test set, it is clear that these model do not fit well on extreme values (very high or very low price). Also, the prediction of regression tree looks like a step function.

## Important variables

```{r,fig.height = 10, fig.width = 8, fig.asp=1, echo=FALSE}
coef = coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
# coef[,1][coef[,1] != 0]
# rownames(coef)

coef_df = 
  tibble(variable = rownames(coef),
         coefficients = coef[,1]) %>% 
  filter(coefficients!=0,
         variable != '(Intercept)') %>% 
  mutate(abs_coef = abs(coefficients),
         positive = ifelse(coefficients>0, TRUE, FALSE),
         variable = as.factor(variable)) %>% 
  arrange(-abs_coef) %>% 
  head(15) 

p1 = 
  coef_df %>% 
    ggplot(aes(x=reorder(variable, abs_coef, median, na.rm = TRUE), y=coefficients, fill=positive)) +
    geom_bar(stat="identity") + coord_flip() + theme(legend.position="none") + labs(x="variable") +
    ggtitle('Variable importance of Lasso')



imp_tree = varImp(tree.fit, scale = TRUE)
imp_tree_df = 
  tibble(variable = rownames(varImp(tree.fit)$importance),
         coefficients = as.vector(varImp(tree.fit)$importance)) %>% 
  filter(coefficients!=0,
         variable != '(Intercept)') %>% 
  mutate(abs_coef = abs(coefficients),
         positive = ifelse(coefficients>0, TRUE, FALSE),
         variable = as.factor(variable)) %>% 
  mutate(variable = variable,
         coefficients = coefficients$Overall) %>% 
  arrange(-abs_coef) %>% 
  head(15) %>% 
  select(variable, coefficients, positive)

p2 =
  imp_tree_df %>% 
      ggplot(aes(x=reorder(variable, -coefficients, median, na.rm = TRUE), y=coefficients, fill=positive)) +
      geom_bar(stat="identity") + theme(legend.position="none",axis.text.x = element_text(angle = 25, size=7.5)) + labs(x="variable", y = 'importance')+
      ggtitle('Variable importance of Decision Tree')


library(gbm)
imp_gbm = varImp(gbm.train)
imp_gbm_df = 
  tibble(variable = rownames(varImp(gbm.train)$importance),
         coefficients = as.vector(varImp(gbm.train)$importance)) %>% 
  filter(coefficients!=0,
         variable != '(Intercept)') %>% 
  mutate(abs_coef = abs(coefficients),
         positive = ifelse(coefficients>0, TRUE, FALSE),
         variable = as.factor(variable)) %>% 
  mutate(variable = variable,
         coefficients = coefficients$Overall) %>% 
  arrange(-abs_coef) %>% 
  head(15) %>% 
  select(variable, coefficients, positive)

p3 = 
  imp_gbm_df %>% 
      ggplot(aes(x=reorder(variable, -coefficients, median, na.rm = TRUE), y=coefficients, fill=positive)) +
      geom_bar(stat="identity") + theme(legend.position="none",axis.text.x = element_text(angle = 25, size=7.5)) + 
  labs(x="variable", y = 'importance')+
      ggtitle('Variable importance of GBM')

# XGB
imp_xgb = varImp(xgb.fit)
imp_xgb_df = 
  tibble(variable = rownames(varImp(xgb.fit)$importance),
         coefficients = as.vector(varImp(xgb.fit)$importance)) %>% 
  filter(coefficients!=0,
         variable != '(Intercept)') %>% 
  mutate(abs_coef = abs(coefficients),
         positive = ifelse(coefficients>0, TRUE, FALSE),
         variable = as.factor(variable)) %>% 
  mutate(variable = variable,
         coefficients = coefficients$Overall) %>% 
  arrange(-abs_coef) %>% 
  head(15) %>% 
  select(variable, coefficients, positive)

p4 = 
  imp_xgb_df %>% 
      ggplot(aes(x=reorder(variable, -coefficients, median, na.rm = TRUE), y=coefficients, fill=positive)) +
      geom_bar(stat="identity") + theme(legend.position="none",axis.text.x = element_text(angle = 25, size=7.5)) + 
  labs(x="variable", y = 'importance')+
      ggtitle('Variable importance of XGBoost')


(p1 + p2) / (p3 + p4)


```

The plot shows the most important 15 variables for each model. For the Lasso model, the variable importance is the coefficients. For all three models, variable year and odometer are among the top three most important variables. From the coefficients of Lasso, the car price is negatively correlated with the odometer and positively related with manufactor year. In addition, they have some shared important variables such as if manufactored by tesla, whether the car type is SUV and whether the car is rear-Wheel drive.

## Model Limitations

* Lasso model does not have good prediction performance in the test set and cross validation
* Regression tree model is explainable but the predictionm performance can be improved
* GBM and XGB models fit data well but the ensemble method is like “black boxes”, we can not clearly explain how it works

### Black box of XGBoost

```{r, include=FALSE}
library(ISLR)
library(caret)
library(vip)
library(pdp)
library(lime)

```

The best model in cross validation and test set is XGBoost. However, XGBoost is a blackbox model. Therefore, We will use partial dependence plots, individual conditional expectation curves and the lime packages to try to give prediction explanations.



#### Partial dependence plots

We have identified some relevant variables in the variable importance part, important ones are year and odometer. 
We will use partial dependence plots to plot the change in the average predicted value as specified feature(s) vary over their marginal distribution.

We can see from the first plot that the bigger the year the higher the predicted price. From the second plot, we can see that higher year and smaller odometer makes predicted price smaller.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
pdp1.rf <- xgb.fit %>%
  partial(pred.var = c("year")) %>%
  autoplot(train = train_df, rug = TRUE)
pdp2.rf <- xgb.fit %>%
  partial(pred.var = c("year","odometer"), chull = TRUE) %>%
  autoplot(train = train_df, rug = TRUE)
grid.arrange(pdp1.rf, pdp2.rf, nrow = 1)

```

#### Individual conditional expectation curves

ICE curves are an extension of partial dependence plots plots but, rather than plot the average marginal effect on the response
variable, we plot the change in the predicted response variable for each observation as we vary each predictor variable. **Plot in the appendix**


#### Lime

lime() functio can also be used to explain the result of the model
on new observations. We create
a compact visual representation of the explanations for each case (6 in total) and label combination in an explanation.
Each extracted feature is shown with its weight, thus giving the importance of the feature in the label prediction. **Plot in the appendix**



# Conclusions

Some conclusions from the data preparation step, analysis step and modeling step:

* We determined the missing pattern for missing variables and used mode imputation for categorical variables, median imputation for continuous variable.
* Some interesting facts about data are discovered such as large number of Tesla cars causing the price histogram to have two vertices. Another fact is the interesting distribution of electric car sales.
* The XGB model has been selected by the best training performance, it has RMSE 4219.979 at training set and RMSE `r as.integer(rmse_gbm)` at the test set while the median car price in the dataset is `r median(df1$price)`.
* Interpreting black-box XGB is done by partial dependence plots, individual conditional expectation curves and local interpretable model-agnostic explainations.
* If more rigorous interpretability is also considered, we should select Regression Tree model because it has acceptable prediction error with RMSE 6054 at cross validation and it has good interpretability.


# Appendix

* All the data, code and documents are in the github, check out the repository **[here](https://github.com/zheyanliu99/craigslist_used_cars.git)**
* The report is within five pages excluding contents, plots and tables, check out the excluded version **[here](https://github.com/zheyanliu99/craigslist_used_cars/blob/main/report_noplot.pdf)**

**ICE curves**

```{r, message=FALSE, echo=FALSE, warning=FALSE}
ice1.rf <- xgb.fit %>%
  partial(pred.var = "year",
  grid.resolution = 100,
  ice = TRUE) %>%
  autoplot(train = train_df, alpha = .1) +
  ggtitle("ICE, not centered")
ice2.rf <- xgb.fit %>%
  partial(pred.var = "year",
  grid.resolution = 100,
  ice = TRUE) %>%
  autoplot(train = train_df, alpha = .1,
  center = TRUE) +
  ggtitle("ICE, centered")
grid.arrange(ice1.rf, ice2.rf, nrow = 1)
```

**Lime**

```{r, message=FALSE, echo=FALSE, warning=FALSE,fig.height = 10, fig.width = 8, fig.asp=1}
explainer.rf <- lime(train_df %>% select(-price), xgb.fit)
new_obs <- (test_df %>% select(-price))[1:6,]
explanation.obs <- explain(new_obs,
                           explainer.rf,
                           n_features = 10)
plot_features(explanation.obs)
```